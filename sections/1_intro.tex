\section{Introduction}

% SECTION 1: What is the problem? Why is it important?

% \todo{hong: for motivation, i wonder if it is better to talk about RAI content work and well-being risks in general in the 1st para. then in the 2nd para, we move on to crowdsourcing platform? Also, don't forget to define key terms we will be using in this paper: RAI content work, wellbeing risks, task designers, crowdsourcing platforms, etc. I think we have all of them in the cscw paper just need move them here too}
\textbf{Responsible AI (RAI) content work} ~\cite{qian2025aura}, such as annotation, moderation, and red teaming of AI systems for safety, is increasingly prevalent on crowdsourcing platforms as AI systems require human oversight for AI safety. The rapid expansion of generative AI systems has only increased demand for RAI content work activities~\cite{StanfordHAI2025AIIndex, GrandViewResearch2024GenerativeAI}. These tasks involve a wide range of potentially harmful content types, from explicit violence and hate speech to more subtle forms of bias and manipulation that require nuanced human judgment~\cite{qian2025aura}. As organizations invest more heavily in AI safety measures, they increasingly rely on RAI content work to train and evaluate AI systems for potential safety risks. Workers who perform RAI content work may encounter material that causes significant psychological distress. Researchers have documented the psychological toll of this exposure on part and full-time RAI content workers ~\cite {roberts2016commercial, qian2025aura}, showing negative effects such as burnout, depression, anxiety, and in extreme cases, post-traumatic stress disorder (PTSD)~\cite{alemadi2024emotional,martinez2024secondary, spence2025content, gebrekidan2024content}. 
\todo{fix this definition framing}

% sentence about how rai content work has been increasingly crowdsourced
We define \textbf{crowdsourcing platforms} as platforms that elicit participation from a general population without providing, following guidance from prior literature that situates crowdwork as a practice of digital outsourcing in which companies use online platforms to assign tasks to unknown suppliers ~\cite{irani2016stories}. 
Crowdworkers may be asked to view tasks with graphic violence, disturbing imagery, or to simulate harmful scenarios designed to probe AI system boundaries~\cite{qian2025locating}. 
Unlike traditional RAI content work roles for part-time and full-time employees that may offer institutional support structures~\cite{qian2025aura, roberts2016commercial, steiger_psychological_2021}, crowdworkers often perform work in isolation, without access to mental health resources or adequate preparation for the content they will encounter ~\cite{schlicher2021flexible, berastegui2021exposure}. 

% SECTION 2: what's been tried/why it didn't work

% \todo{Hong: My common strategy in writing intro is to embed our RW here, in the next three para. Depending on how we eventually organize our RW, I can see that you (1) talk about existing risk disclosure and transparency efforts in crowdsourcing  (including work like Turkopticon, Dynanmo, etc.; also uneven platform practices; can also include RAI transparency framework if needed)--> gap: they are not focusing on well-being risks in RAI content work; (2)talk about inherent tensions across stakeholders and justify our multi-stakeholder approaches.}
\todo{have to add definition of task designers the first time we use it}
% existing risk disclsoure and transparency efforts in crowdsourcing
Current approaches to informing potential workers about the risks of RAI content lack grounding in an empirical understanding of stakeholder needs in the crowdsourcing space. Platforms vary widely in their practices, from providing no disclosure options to offering more structured resources ~\cite{prolific2025participant, prolific2025sensitive, ProlificAPIContentWarning2025}. Even when platforms present more options for risk disclosure, little evidence shows that these mechanisms account for the perspectives of task designers and workers, the stakeholders most directly affected. 

These limitations are compounded by the absence of worker well-being in most AI transparency frameworks. Model cards~\cite{mitchell2019model}, datasheets~\cite{gebru2021datasheets}, and crowdworksheets~\cite{diaz2022crowdworksheets} set important precedents for documenting technical specifications and data collection processes~\cite{gebru2021datasheets, diaz2022crowdworksheets, mitchell2019model}, yet they primarily emphasize model performance and bias rather than the immediate welfare of workers involved in data creation and evaluation. This creates a significant gap between technical transparency, which documents what models do, and work protection transparency, which should inform workers about the risk they face~\cite{bharucha2023content}. The result is a disconnect between the growing establishment of AI transparency tools and the basic safety information needed by workers who make safe AI development possible. 

The design challenge of effective risk disclosure is further complicated by the inherent tension between stakeholder needs and incentives that has been studied in detail in the context of general crowdsourcing tasks~\cite{finnerty2013keep, gaikwad2016boomerang, salehi2015we, irani2013turkopticon, salehi2018ink}. Task designers face pressure to recruit sufficient workers while also meeting ethical obligations to inform them of potential risks~\cite{qian2025locating, finnerty2013keep, kittur2008crowdsourcing, zheng2011task, bragg2018sprout}, while workers need agency to make informed participant decisions and fair compensation for their labor~\cite{irani2013turkopticon, salehi2018ink, martin2014being, silberman2018responsible, toxtli2021quantifying, schlicher2021flexible}, and platforms must balance worker safety with operational efficiency and legal compliance~\cite{gaikwad2016boomerang, xu2017incentivizing, xia2020privacy, allen2018design}. Too little disclosure may expose workers to unexpected harm, while overly detailed warnings may reduce participation rates and compromise research validity. Despite the critical importance of navigating these tradeoffs, little previous literature has explored the unique tensions that may arise in risk disclosure design from the perspectives of the three main stakeholders involved: task designers, workers, and platform representatives for RAI content work. This gap leaves all stakeholders with little empirical guidance for making risk disclosure decisions, potentially undermining worker well-being. 



% SECTION 3: what's new with our approach (why it is great :))
% What are we doing differently? Essentially, flipping what's wrong with prior approaches and turning them into what needs to be done that we're doing. What are the key contributions that make this work different/superior from what's been done before?
To address this gap, we conducted individual 1-1 co-design sessions with 15 task designers, 11 workers, and 3 platform representatives to understand how risk disclosure decisions are made and what tools and workflows could better support effective disclosure practices. Co-design methodology offers a particularly valuable approach for this challenge because it allows us to understand the complex constraints and considerations that shape each individual's perspective while collaboratively exploring solutions that can work for their specific role. By engaging in individual co-design sessions with task designers who use disclosure mechanisms, workers who must interpret and act on disclosure information, and platform representatives who implement and govern these systems, our study captures the full complexities of risk disclosure as a sociotechnical challenge that spans individual and platform levels. 

Our workshops engaged participants in systematically evaluating risk disclosure mechanisms across three key design dimensions: (1) \textbf{Specificity}: ranging from binary warnings to content type categorization to specific content examples; (2) \textbf{Worker agency}: from no opt-out options to informed consent to granular control over exposure levels; and (3) \textbf{Task designer agency}: from generic disclosure templates to manual customization to AI-based disclosure recommendations. 


% we could say "Platfrom representatives were included where possible to understand governance constraints and technical implementation considerations
% or we could say: we ttempted to include platform representatived but faced recruitment chalelnges due to organizational constraints


Specifically, our work addresses the following research questions:
\begin{enumerate}
  \item How do task designers, workers, and platform representatives perceive and approach different risk disclosure mechanisms across key design dimensions? 
  \item What tradeoffs and tensions emerge when stakeholders consider different configurations of risk disclosure mechanisms? 
  % \item What design principles and frameworks can guide the development of risk disclosure systems that balance stakeholder needs across these design dimensions? 
\end{enumerate}
Our multi-stakeholder methodology represents one of the first empirical investigations of risk disclosure design considerations for RAI content work, contributing novel insights into how the intersection of AI development needs, platform constraints, worker protection concerns, and regulatory pressures shapes the disclosure ecosystem in this critical but understudied domain. Through analysis of our co-design session findings, we contribute a multi-stakeholder understanding of risk disclosure in crowdsourced RAI content work, surfacing core tensions across workers, task designers, and platforms, and offering design implications for more accountable and transparent worker protection.
% \todo{Hong: I was playing around to see how to bring out our key contributions here. Maybe something like: We contribute a multi-stakeholder, co-designed understanding of risk disclosure in crowdsourced Responsible AI content work, surfacing core tensions across workers, task designers and platforms, and offering design implications for more accountable and transparent worker protection? 
% }


