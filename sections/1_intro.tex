\section{Introduction}

% SECTION 1: What is the problem? Why is it important?
\textbf{Responsible AI (RAI) content work} ~\cite{qian2025aura}, such as annotation, moderation, and red teaming of AI systems for safety, has become essential as organizations grapple with the risks of increasingly powerful AI systems. The rapid expansion of Generative AI (GenAI) has only heightened this reliance, fueling demand for large-scale human oversight to detect and mitigate safety concerns \cite{StanfordHAI2025AIIndex, GrandViewResearch2024GenerativeAI}. %As organizations invest more heavily in safety measures, RAI content work becomes increasingly essential to help in the training and evaluation of AI systems for potential risks~\cite{udupa2023ethical}. 
Meeting this demand, however, requires human labor to confront harmful material ranging from explicit violence and hate speech to more subtle forms of bias and manipulation that demand nuanced human judgment~\cite{qian2025aura}. Such exposure can cause significant psychological distress, and researchers have documented its toll on both part-time and full-time RAI workers~\cite{roberts2016commercial, qian2025aura}, including burnout, anxiety, depression, and in extreme cases post-traumatic stress disorder (PTSD)~\cite{alemadi2024emotional, martinez2024secondary, spence2025content, gebrekidan2024content}.


Increasingly, organizations externalize RAI content work to \textbf{crowdsourcing platforms}, or platforms that outsource jobs \textit{``to an undefined, generally large group of people through the form of an open call''} ~\cite{howe2006rise, berg2018digital}. These platforms provide the infrastructure for on-demand, distributed labor markets, which AI companies then leverage to scale annotation, moderation, and adversarial testing \cite{udupa2023ethical, mann2025meta, egelman2014crowdsourcing}.
\textbf{Crowdworkers}~\cite{berg2018digital} may face significant \textbf{well-being risks} when taking on tasks that involve graphic violence, disturbing imagery, or to simulate harmful scenarios designed to probe AI system boundaries~\cite{qian2025locating}. 
Unlike part-time or full-time employees in traditional RAI roles, who may receive institutional support such as mental health support or training ~\cite{qian2025aura, roberts2016commercial, steiger_psychological_2021}, crowdworkers often perform this work in isolation, without adequate preparation or access to support systems ~\cite{schlicher2021flexible, berastegui2021exposure}. 


% SECTION 2: what's been tried/why it didn't work
% existing risk disclsoure and transparency efforts in crowdsourcing
Past work in HCI and CSCW has extensively examined the structural condition of crowd work ~\cite{gray2016crowd, silberman2018responsible,salehi2015we}. In particular, foundational efforts like Turkopticon sought to increase transparency by surfacing requester reputations~\cite{irani2013turkopticon}, yet few platform-embedded mechanisms exist to help workers anticipate or avoid emotionally distressing or harmful tasks. In fact, platforms vary widely in their practices, from providing no disclosure options to offering more structured resources ~\cite{prolific2025participant, prolific2025sensitive, ProlificAPIContentWarning2025}. At the same time, prior research has advanced content warnings for social media audiences~\cite{Zhang2024PerceptionsTriggerWarnings, shashirekha2023trigger, bell2025warning}, but it remains unclear how to adapt these insights to workers facing risk disclosure in crowdsourced RAI tasks. Despite growing awareness of well-being risks -- especially in RAI content work -- there is little empirical research on how these risks are communicated and how disclosure practice might be improved. This gap underscores the need to examine \textbf{``risk disclosure''} -- the provision of upfront information about the
nature and potential harms of content tasks ~\cite{bharucha2023content, qian2025aura} -- not merely as an ethical responsibility, but as a ``design challenge'' embedded within sociotechnical systems that often obscure harm.

%Current approaches to informing potential crowdworkers about the risks of RAI content work lack grounding in an empirical understanding of stakeholder needs across the crowdsourcing ecosystem. Platforms vary widely in their practices, from providing no disclosure options to offering more structured resources ~\cite{prolific2025participant, prolific2025sensitive, ProlificAPIContentWarning2025}. Moreover, prior research has established that the burden of disclosing risks is disproportionately placed on the \textbf{task designers} or \textit{``individuals who design and post RAI content work tasks''}~\cite{qian2025locating}. Even when platforms present more options for risk disclosure, little evidence shows that these mechanisms account for the perspectives of task designers and workers, the stakeholders most directly affected. Lessons from prior research that incorporates the perspectives of workers in designing interventions on crowdsourcing platforms have demonstrated the benefits of taking collaborative-design approaches~\cite{irani2016stories, mturk2018aup, silberman2018responsible}.

%Despite the critical role of crowdworkers workers in RAI, the lack of adequate risk disclosure is exacerbated by the omission of worker well-being in most AI transparency frameworks designed to promote ethical AI development. Model cards~\cite{mitchell2019model}, datasheets~\cite{gebru2021datasheets}, and crowdworksheets~\cite{diaz2022crowdworksheets} set important precedents for documenting technical specifications and data collection processes~\cite{gebru2021datasheets, diaz2022crowdworksheets, mitchell2019model}, yet they primarily focus on model performance and bias, often overlooking the immediate welfare of workers involved in data creation and evaluation. This reveals a significant gap in tools for ethical AI development--between technical transparency, which documents what models do, and worker protection transparency, which should inform content workers about the risk they face~\cite{bharucha2023content}. The result is a disconnect between the growing establishment of AI transparency tools and the minimum safety information needed by workers who make safe AI development possible. 

Such a ``design challenge'' is further complicated by the inherent tension between stakeholder needs and incentives ~\cite{finnerty2013keep, gaikwad2016boomerang, salehi2015we, irani2013turkopticon, salehi2018ink}. Prior work has revealed that task designers\footnote{We use the term \textbf{task designers} rather than requesters to more accurately reflect the active and interpretive role individuals play in shaping the structure, framing, and content of crowdsourcing tasks. While requester is the platform-standard term, it implies a transactional relationship that downplays the design decisions and ethical considerations embedded in task creation. Following prior HCI work that emphasizes the creative and normative dimensions of task design~\cite{bragg2018sprout, qian2025locating}, we adopt task designer to foreground their agency and responsibility in shaping worker experience.} face pressure to recruit sufficient workers while also meeting ethical obligations to inform them of potential risks~\cite{qian2025locating, finnerty2013keep, kittur2008crowdsourcing, zheng2011task, bragg2018sprout}, while workers need agency to make informed participant decisions and fair compensation for their labor~\cite{irani2013turkopticon, salehi2018ink, martin2014being, silberman2018responsible, toxtli2021quantifying, schlicher2021flexible}, and platforms must balance worker safety with operational efficiency and legal compliance~\cite{gaikwad2016boomerang, xu2017incentivizing, xia2020privacy, allen2018design}. 
As Fieseler et al ~\cite{fieseler_unfairness_2019} remind us, crowdsourcing platforms mediate a triadic relationship between requester, worker, and platform, yet much of the existing literature focuses only on a dyadic exchange between workers and task designers.
To navigate these tradeoffs, it is therefore critical to examine the unique tensions faced by the three main stakeholders in crowd-based RAI content work -- task designers, workers, and platform -- and to offer guidance that supports ethically grounded decisions aimed at promoting worker well-being. Our aim is to complement worker-centered scholarship by concentrating on the actors who currently shape task structure, disclosure policy, and enforcement.
% Previous research has shown that of the three types of actors in this relationship, workers hold the least decision making power for this reason, we hope to include all three perspectives to have a clear understanding of design opportunities given tensions among all three stakheolders. \todo{add just 1 sentence here about why we are not centering workers' perspective, in case we get some critical reviewers}

% SECTION 3: what's new with our approach (why it is great :))
To address this gap, we conducted one-on-one co-design sessions \cite{tang2024ai,kuo2023understanding} with 15 task designers, 11 workers, and 3 platform representatives to understand how risk disclosure decisions are made and what tools and workflows could better support effective disclosure practices. Our study focused on three key design dimensions: specificity (how detailed or granular the disclosure is), worker agency (the extent to which workers can act on disclosure), and task designer agency (the flexibility and support available to those creating disclosures). We ask the following research questions:\textbf{\textit{ RQ1:}} How do task designers, workers, and platform representatives perceive and approach different risk disclosure mechanisms across key design dimensions? and \textbf{\textit{RQ2: }}What tradeoffs and tensions emerge when stakeholders consider different configurations of risk disclosure mechanisms?  
%\begin{enumerate}
%  \item How do task designers, workers, and platform representatives perceive and approach different risk disclosure mechanisms across key design dimensions? 
 % \item What tradeoffs and tensions emerge when stakeholders consider different configurations of risk disclosure mechanisms?  
%\end{enumerate}

%Co-design methodology offers a particularly valuable approach for this challenge because it allows us to understand the complex constraints and considerations that shape each individual's perspective while collaboratively exploring solutions that can work for their specific role. By engaging in individual co-design sessions with task designers who use disclosure mechanisms, workers who must interpret and act on disclosure information, and platform representatives who implement and govern these systems, our study captures the full complexities of risk disclosure as a sociotechnical challenge that spans individual and platform levels. 

%Our workshops engaged participants in systematically evaluating risk disclosure mechanisms across three key design dimensions: (1) \textbf{Specificity}: ranging from binary warnings to content type categorization to specific content examples; (2) \textbf{Worker agency}: from no opt-out options to informed consent to granular control over exposure levels; and (3) \textbf{Task designer agency}: from generic disclosure templates to manual customization to AI-based disclosure recommendations. Our decision to focus on these dimensions has been informed by prior research on content warnings on social media platforms~\cite {Zhang2024PerceptionsTriggerWarnings, vit2025use} and on the importance of incorporating perspectives of workers~\cite{salehi2018ink,salehi2015we} and task designers~\cite{qian2025locating, gutheim2012fantasktic, bragg2018sprout} in platform interventions. 

Our findings reveal that workers, task designers, and platforms hold divergent expectations about how risk should be communicated in RAI content work. Workers value clear, specific warnings and the ability to make informed decisions; task designers often fear that detailed disclosures will deter participation; and platforms aim to balance protection with policy and scalability. These tensions manifest across key stages of task engagement -- from sign-up and participation to post-task feedback -- and expose structural gaps in responsibility. We identify design opportunities to better support disclosure practices, including adaptive filters, feedback mechanisms, and AI-assisted tools. Ultimately, we argue that risk disclosure is not just a technical feature but a sociotechnical negotiation of power, protection, and participation. We also acknowledge the limits of co-design in this space, and that some categories of RAI content work should not be crowdsourced, in which case it may be better to redesign the pipeline for gathering worker input rather than the disclosure itself.
% \todo{add just one sentence here to preemptive talk about limitations of co-design, what type of RAI work should not be crowdsourced (our 5.4).}
% Our multi-stakeholder methodology represents one of the first empirical investigations of risk disclosure design considerations for RAI content work, contributing novel insights into how the intersection of AI development needs, platform constraints, worker protection concerns, and regulatory pressures shapes the disclosure ecosystem in this critical but understudied domain. Through analysis of our co-design session findings, we contribute a multi-stakeholder understanding of risk disclosure in crowdsourced RAI content work, surfacing core tensions across workers, task designers, and platforms, and offering design implications for more accountable and transparent worker protection.

Our contributions are threefold:

\begin{itemize}
\item \textbf{Empirical insights} into risk disclosure in crowdsourced RAI content work, based on co-design sessions with 15 task designers, 11 workers, and 3 platform representatives. We surface how stakeholder roles, values, and constraints shape disclosure expectations and practices.
\item \textbf{A multi-stakeholder understanding} of risk disclosure as a sociotechnical negotiation, identifying key tensions around specificity, worker agency, and task designer responsibility across the lifecycle of content tasks.
\item \textbf{Design implications} for more accountable and transparent disclosure systems.

\end{itemize}
