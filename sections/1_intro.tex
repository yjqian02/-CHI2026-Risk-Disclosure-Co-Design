\section{Introduction}

% SECTION 1: What is the problem? Why is it important?
\textbf{Responsible AI (RAI) content work} ~\cite{qian2025aura}, such as annotation, moderation, and red teaming of AI systems for safety, has become essential as organizations grapple with the risks of increasingly powerful AI systems. The rapid expansion of generative AI has only heightened this reliance, fueling demand for large-scale human oversight to detect and mitigate safety concerns\cite{StanfordHAI2025AIIndex, GrandViewResearch2024GenerativeAI}. As organizations invest more heavily in safety measures, RAI content work becomes increasingly essential to help in the training and evaluation of AI systems for potential risks~\cite{udupa2023ethical}. Meeting this demand, however, requires workers in organizations to confront harmful material ranging from explicit violence and hate speech to more subtle forms of bias and manipulation that demand nuanced human judgment~\cite{qian2025aura}. Such exposure can cause significant psychological distress, and researchers have documented its toll on both part-time and full-time RAI workers~\cite{roberts2016commercial, qian2025aura}, including burnout, anxiety, depression, and in extreme cases post-traumatic stress disorder (PTSD)~\cite{alemadi2024emotional, martinez2024secondary, spence2025content, gebrekidan2024content}.

% sentence about how rai content work has been increasingly crowdsourced
Increasingly, organizations externalize RAI content work to \textbf{crowdsourcing platforms}, or platforms that outsource jobs \textit{``to an undefined, generally large group of people through the form of an open call''} ~\cite{howe2006rise, berg2018digital}. Such platforms provide AI organizations with on-demand distributed labor markets to scale annotation, moderation, and adversarial testing \cite{udupa2023ethical, mann2025meta, egelman2014crowdsourcing}.
\textbf{Crowdworkers}~\cite{berg2018digital} who then take up these jobs or \textbf{tasks} may face significant \textbf{well-being risks} in being asked to view tasks with graphic violence, disturbing imagery, or to simulate harmful scenarios designed to probe AI system boundaries~\cite{qian2025locating}. 
Unlike traditional RAI content work roles for part-time and full-time employees that may accompany institutional support structures~\cite{qian2025aura, roberts2016commercial, steiger_psychological_2021}, crowdworkers often perform work in isolation, without access to mental health resources or adequate preparation for the content they will encounter ~\cite{schlicher2021flexible, berastegui2021exposure}. 


% \todo{Hong: My common strategy in writing intro is to embed our RW here, in the next three para. Depending on how we eventually organize our RW, I can see that you (1) talk about existing risk disclosure and transparency efforts in crowdsourcing  (including work like Turkopticon, Dynanmo, etc.; also uneven platform practices; can also include RAI transparency framework if needed)--> gap: they are not focusing on well-being risks in RAI content work; (2)talk about inherent tensions across stakeholders and justify our multi-stakeholder approaches.}
% SECTION 2: what's been tried/why it didn't work
% existing risk disclsoure and transparency efforts in crowdsourcing
Current approaches to informing potential crowdworkers about the risks of RAI content work lack grounding in an empirical understanding of stakeholder needs across the crowdsourcing ecosystem. Platforms vary widely in their practices, from providing no disclosure options to offering more structured resources ~\cite{prolific2025participant, prolific2025sensitive, ProlificAPIContentWarning2025}. Moreover, prior research has established that the burden of disclosing risks is disproportionately placed on the \textbf{task designers} or \textit{``individuals who design and post RAI content work tasks''}~\cite{qian2025locating}. Even when platforms present more options for risk disclosure, little evidence shows that these mechanisms account for the perspectives of task designers and workers, the stakeholders most directly affected. Lessons from prior research that incorporates the perspectives of workers in designing interventions on crowdsourcing platforms have demonstrated the benefits of taking collaborative-design approaches~\cite{irani2016stories, mturk2018aup, silberman2018responsible}.

Despite the critical role of crowdworkers workers in RAI, the lack of adequate risk disclosure is exacerbated by the omission of worker well-being in most AI transparency frameworks designed to promote ethical AI development. Model cards~\cite{mitchell2019model}, datasheets~\cite{gebru2021datasheets}, and crowdworksheets~\cite{diaz2022crowdworksheets} set important precedents for documenting technical specifications and data collection processes~\cite{gebru2021datasheets, diaz2022crowdworksheets, mitchell2019model}, yet they primarily focus on model performance and bias, often overlooking the immediate welfare of workers involved in data creation and evaluation. This reveals a significant gap in tools for ethical AI development--between technical transparency, which documents what models do, and worker protection transparency, which should inform content workers about the risk they face~\cite{bharucha2023content}. The result is a disconnect between the growing establishment of AI transparency tools and the minimum safety information needed by workers who make safe AI development possible. 

The design challenge of effective risk disclosure is further complicated by the inherent tension between stakeholder needs and incentives that has been studied in detail in the context of general crowdsourcing tasks~\cite{finnerty2013keep, gaikwad2016boomerang, salehi2015we, irani2013turkopticon, salehi2018ink}. Task designers face pressure to recruit sufficient workers while also meeting ethical obligations to inform them of potential risks~\cite{qian2025locating, finnerty2013keep, kittur2008crowdsourcing, zheng2011task, bragg2018sprout}, while workers need agency to make informed participant decisions and fair compensation for their labor~\cite{irani2013turkopticon, salehi2018ink, martin2014being, silberman2018responsible, toxtli2021quantifying, schlicher2021flexible}, and platforms must balance worker safety with operational efficiency and legal compliance~\cite{gaikwad2016boomerang, xu2017incentivizing, xia2020privacy, allen2018design}. Too little disclosure may expose workers to unexpected harm, while overly detailed warnings may reduce participation rates and compromise research validity. 
% Despite the critical importance of navigating these tradeoffs, little previous literature has explored the unique tensions that may arise in risk disclosure design from the perspectives of the three main stakeholders involved: task designers, workers, and platform representatives for RAI content work. This gap leaves all stakeholders with little empirical guidance for making risk disclosure decisions, potentially undermining worker well-being. 
As Fieseler et al ~\cite{fieseler_unfairness_2019} remind us, crowdsourcing platforms mediate a triadic relationship between requester, worker, and platform, yet much of the existing literature focuses only on a dyadic exchange.
To navigate these tradeoffs in risk disclosure design, it is critical to examine the unique tensions faced by the three main stakeholders in crowd-based RAI content work--task designers, workers, and platform representatives--and to offer guidance that supports ethically grounded decisions aimed at promoting worker well-being.

% SECTION 3: what's new with our approach (why it is great :))
% What are we doing differently? Essentially, flipping what's wrong with prior approaches and turning them into what needs to be done that we're doing. What are the key contributions that make this work different/superior from what's been done before?

% To address this gap in risk disclosure design, we conducted individual 1-1 co-design sessions with 15 task designers, 11 workers, and 3 platform representatives to understand how risk disclosure decisions are made and what tools and workflows could better support effective disclosure practices. As Fieseler et al ~\cite{fieseler_unfairness_2019} remind us, crowdsourcing platforms mediate a triadic relationship between requester, worker, and platform, yet much of the existing literature focuses only on a dyadic exchange; 

Our study addresses the gap in risk disclosure design by foregrounding all three perspectives in the co-design process. We conducted individual 1-1 co-design sessions with 15 task designers, 11 workers, and 3 platform representatives to understand how risk disclosure decisions are made and what tools and workflows could better support effective disclosure practices. Co-design methodology offers a particularly valuable approach for this challenge because it allows us to understand the complex constraints and considerations that shape each individual's perspective while collaboratively exploring solutions that can work for their specific role. By engaging in individual co-design sessions with task designers who use disclosure mechanisms, workers who must interpret and act on disclosure information, and platform representatives who implement and govern these systems, our study captures the full complexities of risk disclosure as a sociotechnical challenge that spans individual and platform levels. 

Our workshops engaged participants in systematically evaluating risk disclosure mechanisms across three key design dimensions: (1) \textbf{Specificity}: ranging from binary warnings to content type categorization to specific content examples; (2) \textbf{Worker agency}: from no opt-out options to informed consent to granular control over exposure levels; and (3) \textbf{Task designer agency}: from generic disclosure templates to manual customization to AI-based disclosure recommendations. Our decision to focus on these dimensions has been informed by prior research on content warnings on social media platforms~\cite {Zhang2024PerceptionsTriggerWarnings, vit2025use} and on the importance of incorporating perspectives of workers~\cite{salehi2018ink,salehi2015we} and task designers~\cite{qian2025locating, gutheim2012fantasktic, bragg2018sprout} in platform interventions. 


Specifically, our work addresses the following research questions:
\begin{enumerate}
  \item How do task designers, workers, and platform representatives perceive and approach different risk disclosure mechanisms across key design dimensions? 
  \item What tradeoffs and tensions emerge when stakeholders consider different configurations of risk disclosure mechanisms?  
\end{enumerate}

Our multi-stakeholder methodology represents one of the first empirical investigations of risk disclosure design considerations for RAI content work, contributing novel insights into how the intersection of AI development needs, platform constraints, worker protection concerns, and regulatory pressures shapes the disclosure ecosystem in this critical but understudied domain. Through analysis of our co-design session findings, we contribute a multi-stakeholder understanding of risk disclosure in crowdsourced RAI content work, surfacing core tensions across workers, task designers, and platforms, and offering design implications for more accountable and transparent worker protection.


