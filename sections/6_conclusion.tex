\section{Conclusion}
The design space of risk disclosure in RAI content work is both essential and under-explored. As platforms increasingly allocate RAI content work tasks to crowdworkers, new mechanisms are urgently needed to ensure workers are not exposed to harmful content without, at the very least, having a robust consent process in place. While prior work has highlighted the importance of content warnings on social media platforms and disclosures for job contracts, far less attention has been paid to how these mechanisms function—or fail—in the context of outsourced labor for responsible AI development.  Our findings reveal the varied and often conflicting goals that shape risk disclosure in crowdsourced AI work. For instance, while task designers worry about scaring off workers with detailed warnings on tasks, workers seek clarity and autonomy in deciding what tasks to accept. Platforms, in turn, attempt to prioritize worker protection while meeting the needs of task designers and compliance with policies. We argue that any intervention in this space must contend with these competing logics. Rather than simplifying or flattening the problem, we advocate for systems that explicitly support negotiation, friction, and choice across stakeholders. Such approaches may offer more sustainable, transparent, and equitable models for risk communication---especially as AI development continues to rely on precarious and distributed forms of human expertise.