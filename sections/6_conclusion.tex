\section{Conclusion}
\todo{may want to add a more formal limitation and future work section before discussion}
The design space of risk disclosure in RAI content work is both essential and under-explored. As platforms increasingly allocate RAI content work tasks to crowdworkers, new mechanisms are urgently needed to ensure workers are not unknowingly exposed to harm. While prior work has highlighted the importance of content warnings on social media platforms and disclosures for job contracts, far less attention has been paid to how these mechanisms function—or fail—in the context of outsourced labor for responsible AI development. To address this gap, we conducted co-design sessions with 29 participants, including crowdworkers, task designers, and platform representatives. Our findings reveal the varied and often conflicting goals that shape risk disclosure in crowdsourced AI work. For instance, while task designers worry about scaring off workers with detailed warnings on tasks, workers seek clarity and autonomy in deciding what tasks to accept. Platforms, in turn, should prioritize worker protection with meeting the needs of task designers and compliance with policies. We argue that any intervention in this space must contend with these competing logics. Rather than simplifying or flattening the problem, we advocate for systems that explicitly support negotiation, friction, and choice across stakeholders. Such approaches may offer more sustainable, transparent, and equitable models for risk communication---especially as AI development continues to rely on precarious and distributed forms of human expertise.