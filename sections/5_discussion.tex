\section{Discussion}

% ideas I have
% opportunities for design/evaluation on crowdsourcing platforms and the need for this information to be widely shared
% parallels rai crowd workers are displaying compared to what's found in the literature --> the question of whether it's ethical to keep crowdsourcing in the future

\subsection{Design Implications}
% here we talk about specific design implications from the findings

\subsection{Negotiated Responsibility in Risk Disclosure}
% discussion of risk disclosure being a site of negotiated responsibility between our 3 groups. Our findings show how these actors hold different assumptions about who is responsible for identifying and mitigating harm. Acknowledging this dynamic allows us to shift our design question from how risk should be disclosed to who decides, when, and on what basis? 
Our study reveals that risk disclosure in AI data work is a socially negotiated process among workers, requesters, and platforms. We confirm previous work that found these groups hold distinct and at times conflicting assumptions about who should identify, assess, and communicate risks \cite{fieseler_unfairness_2019}. Workers often assume that task requesters or platforms have already vetted tasks for harm, while task designers may assume that workers bear responsibility for self-selecting out of sensitive content. Platforms, meanwhile, frequently promote a vision of shared responsibility while retaining control over infrastructure and moderation policies. These mismatches lead to practical gaps in risk disclosure and reinforce structural ambiguities around accountability \cite{Suchman2002LocatedAccountabilities, widder_dislocated_2023}.


This ambiguity invites us to reframe the design challenge. Rather than simply asking how risk should be disclosed, we must interrogate who decides, when, and on what basis. These questions draw attention to power asymmetries embedded in crowdsourcing platforms. Workers have limited opportunities to contest or shape disclosure practices, even as they bear the brunt of poorly disclosed risks. Task designers may want to warn workers, but are constrained by platform guidelines or unsure of what they are permitted to disclose. Platforms can offer guidelines or tools, but are often opaque about how risks are evaluated or escalated internally~\cite{roberts2019behind}.

By conceptualizing risk disclosure as a site of negotiated responsibility, we can open new design directions. Future tools might aim not just to standardize disclosure formats, but to mediate disagreement and dialogue across stakeholder boundaries \cite{fieseler_unfairness_2019}. For example, platforms could surface discrepancies in risk perception between task creators and workers, or allow workers to annotate disclosures with feedback. Designing for this kind of deliberation requires acknowledging that disclosure is a form of boundary crossing~\cite{Suchman2002LocatedAccountabilities}, not simply a transfer of information, but a process shaped by contested expertise, values, and institutional constraints.

Moreover, understanding these negotiations requires us to move beyond individualistic framings of responsibility. Instead, we can draw on theories of *distributed* or *relational* accountability, which emphasize how ethical obligations are produced through collective, situated practice~\todo{add citation on relational ethics or actor-network theory}. Such a lens complicates the assumption that risks can be fully known or managed in advance, and suggests the need for systems that are reflexive and adaptive over time.


\subsection{Beyond Risk Disclosure}
% where we can talk about workers needing more support (helpline isn't enough) and also what are platform gonna do if they provide a number of a product manager you can call? unrealistic?
While task-level risk disclosures are a critical starting point, they are only one component in a broader ecology of worker well-being. Our findings reveal that workers, even when appreciative of disclosure efforts, continue to face harm that is structural, cumulative, and poorly addressed by current platform support. Many participants described psychological strain from sustained exposure to distressing content, difficulty disengaging from harmful tasks, and a lack of avenues for recovery. Helplines and automated filters offer some relief, but fall short of meeting the scale and complexity of workers’ needs~\todo{add citation on emotional labor and crowdworker burnout}.

In particular, workers expressed a desire for more robust pre-task support. This includes not only clearer disclosures, but also access to training and reflection tools that could help them assess personal thresholds for harm and prepare emotionally for the work. Existing research on content moderation and trauma-informed design points to promising strategies here—such as onboarding experiences, harm typologies, and peer knowledge sharing—that could be adapted for the crowdsourcing context~\todo{add citation on moderation training or trauma-informed interface design}.

Post-task care is an even more underexplored frontier. Some workers suggested mechanisms such as debriefing spaces, access to mental health resources, or the ability to pause or escalate after encountering disturbing content. However, the logistics of such support remain unclear: who would provide it, under what funding model, and with what safeguards? In an industry defined by just-in-time labor and minimal worker protections, offering meaningful well-being resources challenges the very business models that platforms depend on~\cite{gray2019ghost}.

Moreover, the workers most vulnerable to psychological harm are often those with the least power to request accommodations. Crowdworkers span geographies, languages, and socioeconomic contexts—factors that mediate not only their exposure to harm but also their ability to access care. Designing for worker well-being thus requires a deeply intersectional approach, one that foregrounds structural inequalities rather than treating harm as a purely individual experience~\todo{add citation on intersectional HCI or global crowdwork}.

\subsection{Question of whether we should crowdsource RAI content work}
% cite implication to not design paper
% what type of task should be crowdsourced
% should be crowdsource at all
% can directly call out our limitation as a co-design study here, with power imbalance of workers
% from Hong this report has definitions of platform: https://www.ilo.org/sites/default/files/wcmsp5/groups/public/%40dgreports/%40dcomm/%40publ/documents/publication/wcms_645337.pdf
% - we focus on the complexities of this design space, but there is an itneresting question about whether it's effective or ethical to crowdsource this work
% - maybe we can crowdsource, but we don't make this into microtasks
% - in terms of what is effective, this does align with what our task designers want and some of our findings showed that some workers are open to this too
% - of course this will take away power from platforms
% - in terms of the ethics (cite literature) 
% this is sort of a policy implications section

Our findings also surface a deeper question: \textbf{Should we crowdsource risky or sensitive AI content work in the first place?} While our study focuses on improving the design of risk disclosures, it cannot sidestep broader concerns about whether the task environment itself is appropriate or just. As prior literature has shown, microtasking can fragment judgment, mask ethical complexity, and devalue the expertise required to make nuanced content decisions~\todo{add citation on fragmentation and judgment in crowd work}. These concerns are amplified in the context of RAI development, where annotation decisions can shape downstream system behavior.

Several task designers in our study justified crowdsourcing on the basis of diversity and scalability, noting that crowdworkers provide valuable perspectives and can be engaged more flexibly than full-time staff. Some workers echoed this, citing financial need or interest in meaningful participation. Yet the same workers also described feeling isolated, underpaid, and unsupported when engaging with emotionally fraught tasks. This suggests that current practices may extract value from worker insight without adequately compensating for the associated risks~\todo{add citation on labor extraction and affective value}.

Rather than rejecting crowdsourcing altogether, one alternative is to rethink how it is done. For example, longer-form or collaborative annotation formats might give workers greater context and control. Task structures could emphasize care and deliberation over speed, perhaps shifting away from performance-based incentives and toward reflective judgment. Some of these ideas mirror trends in community moderation or participatory research, where contributors are treated as co-creators rather than disposable labor~\todo{add citation on participatory AI or collaborative annotation}.

However, such changes would likely challenge platform logics and economic incentives. Implementing slower, more dialogic forms of labor requires reconfiguring not only interface design, but also compensation structures, labor policy, and institutional accountability. These are not merely technical or logistical challenges, but ethical ones. We therefore join prior scholars in calling for greater scrutiny of whether certain types of work should be crowdsourced at all—and under what conditions it can be considered just, humane, and sustainable~\todo{add citation on just data labor or AI ethics and labor}. 
